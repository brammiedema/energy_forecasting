{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-aebfb78fe0f1b1d1' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'sarah-bucket' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-west1-a' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "Build_model_task.ipynb\tCNN_W2P_model\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file:///content/datalab/energy_forcasing/sarah_data.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/  1.2 MiB]                                                \r",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \r\n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "## move the csv file to the bucket\n",
    "%%bash\n",
    "\n",
    "gsutil cp -p /content/datalab/energy_forcasing/sarah_data.csv gs://sarah-bucket/sarah_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%writefile CNN_W2P_model/trainer/model.py\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def train(output_dir, batch_size, learning_rate):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  \n",
    "  # Read dataset and split into train and eval\n",
    "  df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n",
    "  df['num_rooms'] = df['total_rooms'] / df['households']\n",
    "  msk = np.random.rand(len(df)) < 0.8\n",
    "  traindf = df[msk]\n",
    "  evaldf = df[~msk]\n",
    "\n",
    "  # Train and eval input functions\n",
    "  SCALE = 100000\n",
    "  \n",
    "  train_input_fn = tf.estimator.inputs.pandas_input_fn(x = traindf[[\"num_rooms\"]],\n",
    "                                                       y = traindf[\"median_house_value\"] / SCALE,  # note the scaling\n",
    "                                                       num_epochs = None,\n",
    "                                                       batch_size = batch_size, # note the batch size\n",
    "                                                       shuffle = True)\n",
    "  \n",
    "  eval_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[[\"num_rooms\"]],\n",
    "                                                      y = evaldf[\"median_house_value\"] / SCALE,  # note the scaling\n",
    "                                                      num_epochs = 1,\n",
    "                                                      batch_size = len(evaldf),\n",
    "                                                      shuffle=False)\n",
    "  \n",
    "  # Define feature columns\n",
    "  features = [tf.feature_column.numeric_column('num_rooms')]\n",
    "  \n",
    "  def train_and_evaluate(output_dir):\n",
    "    # Compute appropriate number of steps\n",
    "    num_steps = (len(traindf) / batch_size) / learning_rate  # if learning_rate=0.01, hundred epochs\n",
    "\n",
    "    # Create custom optimizer\n",
    "    myopt = tf.train.FtrlOptimizer(learning_rate = learning_rate) # note the learning rate\n",
    "\n",
    "    # Create rest of the estimator as usual\n",
    "    estimator = tf.estimator.LinearRegressor(model_dir = output_dir, \n",
    "                                             feature_columns = features, \n",
    "                                             optimizer = myopt)\n",
    "    #Add rmse evaluation metric\n",
    "    def rmse(labels, predictions):\n",
    "      pred_values = tf.cast(predictions['predictions'],tf.float64)\n",
    "      return {'rmse': tf.metrics.root_mean_squared_error(labels*SCALE, pred_values*SCALE)}\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator,rmse)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn,\n",
    "                                        max_steps = num_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn,\n",
    "                                      steps = None)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "  # Run the training\n",
    "  shutil.rmtree(output_dir, ignore_errors=True) # start fresh each time\n",
    "  train_and_evaluate(output_dir)\n",
    "    \n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type = float, \n",
    "      default = 0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type = int, \n",
    "      default = 30\n",
    "  ),\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help = 'GCS location to write checkpoints and export models.',\n",
    "      required = True\n",
    "  )\n",
    "  args = parser.parse_args()\n",
    "  print(\"Writing checkpoints to {}\".format(args.job_dir))\n",
    "  train(args.job_dir, args.batch_size, args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(row):\n",
    "      #row is a string tensor containing the contents of one row\n",
    "      features = tf.decode_csv(row, record_defaults = DEFAULTS) #string tensor -> list of 50 rank 0 float tensors\n",
    "      label = features.pop() #remove last feature and use as label\n",
    "      features = tf.stack(features) #list of rank 0 tensors -> single rank 1 tensor\n",
    "      return {TIMESERIES_COL: features}, label\n",
    "\n",
    "    # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "    dataset = tf.data.Dataset.list_files(filename) \n",
    "    # Read in data from files\n",
    "    dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "    # Parse text lines as comma-separated values (CSV)\n",
    "    dataset = dataset.map(decode_csv)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # loop indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
