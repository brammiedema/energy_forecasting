{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-aebfb78fe0f1b1d1' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'sarah-bucket' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-west1-a' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "Build_model_task.ipynb\tCNN_W2P_model\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/datalab/energy_forcasing/sarah_data.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "## move the csv file to the bucket\n",
    "!gsutil cp -p /content/datalab/energy_forcasing/sarah_data.csv gs://sarah-bucket/sarah_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf CNN_W2P_model/trainer\n",
    "mkdir CNN_W2P_model/trainer\n",
    "touch CNN_W2P_model/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TIMESERIES_COL = 'height'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "SEQ_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "\n",
    "def init(hparams):\n",
    "  global SEQ_LEN, DEFAULTS, N_INPUTS\n",
    "  SEQ_LEN =  hparams['sequence_length']\n",
    "  DEFAULTS = [[0.0] for x in xrange(0, SEQ_LEN)]\n",
    "  N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "  X = features[TIMESERIES_COL]\n",
    "  predictions = tf.layers.dense(X, 1, activation=None)\n",
    "  return predictions\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "  X = features[TIMESERIES_COL]\n",
    "  h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
    "  h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
    "  predictions = tf.layers.dense(h2, 1, activation=None) # linear output: regression\n",
    "  return predictions\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "  X = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1]) # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "  c1 = tf.layers.conv1d(X, filters=N_INPUTS//2,\n",
    "                          kernel_size=3, strides=1, \n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "  p1 = tf.layers.max_pooling1d(c1,pool_size=2, strides=2) \n",
    "    \n",
    "  c2 = tf.layers.conv1d(p1, filters=N_INPUTS//2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu) \n",
    "  p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "    \n",
    "  outlen = p2.shape[1]*p2.shape[2]\n",
    "  c2flat = tf.reshape(p2, [-1, outlen])\n",
    "  h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "  predictions = tf.layers.dense(h1, 1, activation=None) # linear output: regression\n",
    "  return predictions\n",
    "\n",
    "def lstm_model(features, mode, params):\n",
    "  LSTM_SIZE = N_INPUTS//3  # size of the internal state in each of the cells\n",
    "\n",
    "  # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "  x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "  # 2. configure the RNN\n",
    "  lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)\n",
    "  outputs, _ = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "  outputs = outputs[:, (N_INPUTS-1):, :]  # last cell only\n",
    "\n",
    "  # 3. flatten lstm output and pass through a dense layer\n",
    "  lstm_flat = tf.reshape(outputs, [-1, lstm_cell.output_size])\n",
    "  h1 = tf.layers.dense(lstm_flat, N_INPUTS//2, activation=tf.nn.relu)\n",
    "  predictions = tf.layers.dense(h1, 1, activation=None) # (?, 1)\n",
    "  return predictions\n",
    "\n",
    "# 2-layer LSTM\n",
    "def lstm2_model(features, mode, params):\n",
    "  # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "  x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    " \n",
    "  # 2. configure the RNN\n",
    "  lstm_cell1 = rnn.BasicLSTMCell(N_INPUTS*2, forget_bias=1.0)\n",
    "  lstm_cell2 = rnn.BasicLSTMCell(N_INPUTS//2, forget_bias=1.0)\n",
    "  lstm_cells = rnn.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "  outputs, _ = tf.nn.dynamic_rnn(lstm_cells, x, dtype=tf.float32)\n",
    "  outputs = outputs[:, (N_INPUTS-1):, :] # last one only\n",
    "\n",
    "  # 3. flatten lstm output and pass through a dense layer\n",
    "  lstm_flat = tf.reshape(outputs, [-1, lstm_cells.output_size])\n",
    "  h1 = tf.layers.dense(lstm_flat, lstm_cells.output_size//2, activation=tf.nn.relu)\n",
    "  predictions = tf.layers.dense(h1, 1, activation=None) # (?, 1)\n",
    "  return predictions\n",
    "\n",
    "# create N-1 predictions\n",
    "def lstmN_model(features, mode, params):\n",
    "  # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "  x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    " \n",
    "  # 2. configure the RNN\n",
    "  lstm_cell1 = rnn.BasicLSTMCell(N_INPUTS*2, forget_bias=1.0)\n",
    "  lstm_cell2 = rnn.BasicLSTMCell(N_INPUTS//2, forget_bias=1.0)\n",
    "  lstm_cells = rnn.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "  outputs, _ = tf.nn.dynamic_rnn(lstm_cells, x, dtype=tf.float32)\n",
    "\n",
    "  # 3. make lstm output a 2D matrix and pass through a dense layer\n",
    "  # so that the dense layer is shared for all outputs\n",
    "  lstm_flat = tf.reshape(outputs, [-1, N_INPUTS, lstm_cells.output_size])\n",
    "  h1 = tf.layers.dense(lstm_flat, lstm_cells.output_size, activation=tf.nn.relu)\n",
    "  h2 = tf.layers.dense(h1, lstm_cells.output_size//2, activation=tf.nn.relu)\n",
    "  predictions = tf.layers.dense(h2, 1, activation=None) # (?, N_INPUTS, 1)\n",
    "  predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "  return predictions\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(row):\n",
    "      #row is a string tensor containing the contents of one row\n",
    "      features = tf.decode_csv(row, record_defaults = DEFAULTS) #string tensor -> list of 50 rank 0 float tensors\n",
    "      label = features.pop() #remove last feature and use as label\n",
    "      features = tf.stack(features) #list of rank 0 tensors -> single rank 1 tensor\n",
    "      return {TIMESERIES_COL: features}, label\n",
    "\n",
    "    # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "    dataset = tf.data.Dataset.list_files(filename) \n",
    "    # Read in data from files\n",
    "    dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "    # Parse text lines as comma-separated values (CSV)\n",
    "    dataset = dataset.map(decode_csv)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # loop indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "  \n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "   labels = tf.expand_dims(labels,-1) #rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "   if predictions.shape[1] == 1:\n",
    "      loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "      rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "      return loss, rmse\n",
    "   else:\n",
    "      # one prediction for every input in sequence\n",
    "      # get 1-N of (x + label)\n",
    "      labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
    "      labelsN = labelsN[:, 1:]\n",
    "      # loss is computed from the last 1/3 of the series\n",
    "      N = (2 * N_INPUTS) // 3\n",
    "      loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "      # rmse is computed from last prediction and last label\n",
    "      lastPred = predictions[:, -1]\n",
    "      rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "      return loss, rmse\n",
    "\n",
    "# create the inference model\n",
    "def sequence_regressor(features, labels, mode, params):\n",
    "  # 1. run the appropriate model\n",
    "  model_functions = {\n",
    "      'linear':linear_model,\n",
    "      'dnn':dnn_model,\n",
    "      'cnn':cnn_model,\n",
    "      'lstm':lstm_model,\n",
    "      'lstm2':lstm2_model,\n",
    "      'lstmN':lstmN_model}\n",
    "  model_function = model_functions[params['model']]\n",
    "  predictions = model_function(features, mode, params)\n",
    "\n",
    "  # 2. loss function, training/eval ops\n",
    "  loss = None\n",
    "  train_op = None\n",
    "  eval_metric_ops = None\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "     loss, rmse = compute_errors(features, labels, predictions)\n",
    "\n",
    "     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # this is needed for batch normalization, but has no effect otherwise\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "           # 2b. set up training operation\n",
    "           train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.train.get_global_step(),\n",
    "              learning_rate=params['learning_rate'],\n",
    "              optimizer=\"Adam\")\n",
    " \n",
    "     # 2c. eval metric\n",
    "     eval_metric_ops = {\n",
    "      \"rmse\": rmse\n",
    "     }\n",
    "\n",
    "  # 3. Create predictions\n",
    "  if predictions.shape[1] != 1:\n",
    "     predictions = predictions[:, -1] # last predicted value\n",
    "  predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "  # 4. return EstimatorSpec\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions_dict,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=eval_metric_ops,\n",
    "      export_outputs={\n",
    "          'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "  )\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "  get_train = read_dataset(hparams['train_data_path'],\n",
    "                                   tf.estimator.ModeKeys.TRAIN,\n",
    "                                   hparams['train_batch_size'])\n",
    "  get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                                   tf.estimator.ModeKeys.EVAL,\n",
    "                                   1000)\n",
    "  estimator = tf.estimator.Estimator(model_fn = sequence_regressor,\n",
    "                                   params = hparams,\n",
    "                                   config=tf.estimator.RunConfig(\n",
    "                                      save_checkpoints_secs =\n",
    "                                        hparams['min_eval_frequency']),\n",
    "                                   model_dir = output_dir)\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn = get_train,\n",
    "                                   max_steps = hparams['train_steps'])\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn = get_valid,\n",
    "                                  steps = None,\n",
    "                                  exporters = exporter,\n",
    "                                  start_delay_secs = hparams['eval_delay_secs'],\n",
    "                                  throttle_secs = hparams['min_eval_frequency'])\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--sequence_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\r\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "## /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/bin/python: No module named CNN_W2P_model\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "DATADIR=/content/datalab/energy_forcasing/Sarah\n",
    "OUTDIR=/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "rm -rf $OUTDIR\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=CNN_W2P_model.task \\\n",
    "   --package-path=${PWD}/CNN_W2P_model \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/sarah_data.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/sarah_data.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=lstm --train_steps=10 --sequence_length=$SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
