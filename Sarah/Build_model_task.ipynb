{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-aebfb78fe0f1b1d1' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'sarah-bucket' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-west1-a' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version\n",
    "\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "BIG_q.ipynb\t\tCNN_W2P_model  Energy_ts       train-1.csv  valid-1.csv\n",
      "Build_model_task.ipynb\tdata\t       sarah_data.csv  trained\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying file:///content/datalab/energy_forcasing/Sarah/sarah_data.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "## move the csv file to the bucket ~ you rly dont need to do this\n",
    "!gsutil cp -p /content/datalab/energy_forcasing/Sarah/sarah_data.csv gs://sarah-bucket/sarah_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_direction_100m</th>\n",
       "      <th>temperature</th>\n",
       "      <th>air_density</th>\n",
       "      <th>pressure</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>radiation</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.799901</td>\n",
       "      <td>189.582975</td>\n",
       "      <td>14.920355</td>\n",
       "      <td>1.132338</td>\n",
       "      <td>935.286695</td>\n",
       "      <td>0.117364</td>\n",
       "      <td>4.872509</td>\n",
       "      <td>225.250182</td>\n",
       "      <td>3.220986</td>\n",
       "      <td>189.082629</td>\n",
       "      <td>50.153506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.964154</td>\n",
       "      <td>58.126639</td>\n",
       "      <td>9.626527</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>6.105083</td>\n",
       "      <td>0.245561</td>\n",
       "      <td>2.428587</td>\n",
       "      <td>266.299815</td>\n",
       "      <td>1.319704</td>\n",
       "      <td>56.712796</td>\n",
       "      <td>12.448392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.372222</td>\n",
       "      <td>47.883333</td>\n",
       "      <td>-6.044444</td>\n",
       "      <td>1.036111</td>\n",
       "      <td>913.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>50.538889</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.534722</td>\n",
       "      <td>142.598611</td>\n",
       "      <td>7.540278</td>\n",
       "      <td>1.106111</td>\n",
       "      <td>932.034722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.284722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.294444</td>\n",
       "      <td>143.750000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.477778</td>\n",
       "      <td>191.908333</td>\n",
       "      <td>14.213889</td>\n",
       "      <td>1.131667</td>\n",
       "      <td>934.761111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.327778</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2.961111</td>\n",
       "      <td>191.208333</td>\n",
       "      <td>50.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.544444</td>\n",
       "      <td>232.043056</td>\n",
       "      <td>21.062500</td>\n",
       "      <td>1.160556</td>\n",
       "      <td>937.805556</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.627778</td>\n",
       "      <td>416.422222</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>230.865278</td>\n",
       "      <td>59.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.044444</td>\n",
       "      <td>329.544444</td>\n",
       "      <td>41.044444</td>\n",
       "      <td>1.229444</td>\n",
       "      <td>961.238889</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>16.705556</td>\n",
       "      <td>954.288889</td>\n",
       "      <td>9.738889</td>\n",
       "      <td>327.711111</td>\n",
       "      <td>85.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wind_speed_100m  wind_direction_100m  temperature  air_density  \\\n",
       "count      6646.000000          6646.000000  6646.000000  6646.000000   \n",
       "mean          4.799901           189.582975    14.920355     1.132338   \n",
       "std           1.964154            58.126639     9.626527     0.039333   \n",
       "min           1.372222            47.883333    -6.044444     1.036111   \n",
       "25%           3.534722           142.598611     7.540278     1.106111   \n",
       "50%           4.477778           191.908333    14.213889     1.131667   \n",
       "75%           5.544444           232.043056    21.062500     1.160556   \n",
       "max          15.044444           329.544444    41.044444     1.229444   \n",
       "\n",
       "          pressure  precipitation    wind_gust    radiation   wind_speed  \\\n",
       "count  6646.000000    6646.000000  6646.000000  6646.000000  6646.000000   \n",
       "mean    935.286695       0.117364     4.872509   225.250182     3.220986   \n",
       "std       6.105083       0.245561     2.428587   266.299815     1.319704   \n",
       "min     913.800000       0.000000     1.194444     0.000000     0.855556   \n",
       "25%     932.034722       0.000000     3.284722     0.000000     2.294444   \n",
       "50%     934.761111       0.000000     4.327778    95.000000     2.961111   \n",
       "75%     937.805556       0.100000     5.627778   416.422222     3.850000   \n",
       "max     961.238889       1.700000    16.705556   954.288889     9.738889   \n",
       "\n",
       "       wind_direction        price  \n",
       "count     6646.000000  6646.000000  \n",
       "mean       189.082629    50.153506  \n",
       "std         56.712796    12.448392  \n",
       "min         50.538889     4.000000  \n",
       "25%        143.750000    43.000000  \n",
       "50%        191.208333    50.890000  \n",
       "75%        230.865278    59.950000  \n",
       "max        327.711111    85.050000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_COLUMNS = ['prediction_date', 'wind_speed_100m', \n",
    "  'wind_direction_100m', \n",
    "  'temperature', \n",
    "  'air_density',\n",
    "  'pressure', 'precipitation', 'wind_gust', 'radiation', \n",
    "  'wind_speed', 'wind_direction', 'price']\n",
    "FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\n",
    "LABEL = CSV_COLUMNS[0]\n",
    "\n",
    "df5 = pd.read_csv('/content/datalab/energy_forcasing/Sarah/sarah_data.csv', header = None, names = CSV_COLUMNS)\n",
    "df5.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6646, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ii in range(10):\n",
    "#   N = 7\n",
    "#  SEQ = df5.price[(0+N*ii):(N+N*ii)]\n",
    "np.random.seed(1) \n",
    "# np.random.uniform(0,6646-N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(np.random.uniform(0,6646-N),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 24 # Sequence of 1 week\n",
    "with open('./data/timeserie_price_daily_train.csv', 'w') as f:\n",
    "    to_csv = ''\n",
    "    for idx, p in enumerate(df5['price']):\n",
    "        to_csv += str(p) + ','\n",
    "        if (idx + 1) % SEQ_LEN == 0:\n",
    "            f.write(to_csv[:-1] + '\\n')\n",
    "            to_csv = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int(round(np.random.uniform(0,6646-N),0))\n",
    "#6646/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(filename, N, J):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    start = 0\n",
    "    end = J\n",
    "    for ii in xrange(0, N):\n",
    "      nn = int(round(np.random.uniform(low = 0, high = 6466-N),0))\n",
    "      #print((nn),(nn+J))\n",
    "      seq = df5.price[(nn):(nn+J)]\n",
    "      #seq = df5.price[start: end]\n",
    "      #start += STRIDE\n",
    "      #end += STRIDE\n",
    "      line = \",\".join(map(str, seq))\n",
    "      ofp.write(line + '\\n')\n",
    "      #print(len(seq))\n",
    "      #if len(seq) != J:\n",
    "      #  break\n",
    "\n",
    "import os\n",
    "try:\n",
    "  os.makedirs('data/CNN_W2P_model/')\n",
    "except OSError:\n",
    "  pass\n",
    "to_csv('data/CNN_W2P_model/train-1.csv', N = 100, J = 50)  # 1000 sequences\n",
    "to_csv('data/CNN_W2P_model/valid-1.csv', N = 150, J = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/CNN_W2P_model/train-1.csv', header = None)\n",
    "df2.head()\n",
    "#df2.describe()\n",
    "# df2.count()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.50</td>\n",
       "      <td>39.75</td>\n",
       "      <td>67.10</td>\n",
       "      <td>67.10</td>\n",
       "      <td>66.20</td>\n",
       "      <td>65.51</td>\n",
       "      <td>50.48</td>\n",
       "      <td>59.23</td>\n",
       "      <td>28.30</td>\n",
       "      <td>38.40</td>\n",
       "      <td>...</td>\n",
       "      <td>63.50</td>\n",
       "      <td>68.10</td>\n",
       "      <td>50.84</td>\n",
       "      <td>64.10</td>\n",
       "      <td>39.41</td>\n",
       "      <td>45.10</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.74</td>\n",
       "      <td>69.10</td>\n",
       "      <td>43.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.55</td>\n",
       "      <td>54.74</td>\n",
       "      <td>30.00</td>\n",
       "      <td>46.86</td>\n",
       "      <td>39.25</td>\n",
       "      <td>48.50</td>\n",
       "      <td>43.49</td>\n",
       "      <td>59.57</td>\n",
       "      <td>40.13</td>\n",
       "      <td>45.00</td>\n",
       "      <td>...</td>\n",
       "      <td>30.00</td>\n",
       "      <td>65.10</td>\n",
       "      <td>57.20</td>\n",
       "      <td>30.00</td>\n",
       "      <td>50.76</td>\n",
       "      <td>47.08</td>\n",
       "      <td>45.1</td>\n",
       "      <td>14.84</td>\n",
       "      <td>60.10</td>\n",
       "      <td>60.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.99</td>\n",
       "      <td>59.85</td>\n",
       "      <td>41.21</td>\n",
       "      <td>39.50</td>\n",
       "      <td>39.82</td>\n",
       "      <td>60.10</td>\n",
       "      <td>38.40</td>\n",
       "      <td>52.10</td>\n",
       "      <td>51.90</td>\n",
       "      <td>50.78</td>\n",
       "      <td>...</td>\n",
       "      <td>68.52</td>\n",
       "      <td>4.00</td>\n",
       "      <td>34.77</td>\n",
       "      <td>50.85</td>\n",
       "      <td>54.04</td>\n",
       "      <td>55.33</td>\n",
       "      <td>57.9</td>\n",
       "      <td>61.00</td>\n",
       "      <td>41.38</td>\n",
       "      <td>44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57.80</td>\n",
       "      <td>54.32</td>\n",
       "      <td>59.10</td>\n",
       "      <td>60.10</td>\n",
       "      <td>60.00</td>\n",
       "      <td>64.10</td>\n",
       "      <td>57.98</td>\n",
       "      <td>36.10</td>\n",
       "      <td>48.10</td>\n",
       "      <td>52.64</td>\n",
       "      <td>...</td>\n",
       "      <td>50.30</td>\n",
       "      <td>64.03</td>\n",
       "      <td>48.96</td>\n",
       "      <td>58.30</td>\n",
       "      <td>50.10</td>\n",
       "      <td>63.52</td>\n",
       "      <td>67.1</td>\n",
       "      <td>47.38</td>\n",
       "      <td>62.93</td>\n",
       "      <td>71.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60.83</td>\n",
       "      <td>60.43</td>\n",
       "      <td>35.50</td>\n",
       "      <td>50.72</td>\n",
       "      <td>36.66</td>\n",
       "      <td>49.00</td>\n",
       "      <td>42.60</td>\n",
       "      <td>64.00</td>\n",
       "      <td>52.45</td>\n",
       "      <td>6.33</td>\n",
       "      <td>...</td>\n",
       "      <td>32.01</td>\n",
       "      <td>28.50</td>\n",
       "      <td>42.94</td>\n",
       "      <td>50.20</td>\n",
       "      <td>43.98</td>\n",
       "      <td>55.07</td>\n",
       "      <td>63.0</td>\n",
       "      <td>38.50</td>\n",
       "      <td>65.01</td>\n",
       "      <td>67.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9   \\\n",
       "0  39.50  39.75  67.10  67.10  66.20  65.51  50.48  59.23  28.30  38.40   \n",
       "2  56.55  54.74  30.00  46.86  39.25  48.50  43.49  59.57  40.13  45.00   \n",
       "4  19.99  59.85  41.21  39.50  39.82  60.10  38.40  52.10  51.90  50.78   \n",
       "6  57.80  54.32  59.10  60.10  60.00  64.10  57.98  36.10  48.10  52.64   \n",
       "7  60.83  60.43  35.50  50.72  36.66  49.00  42.60  64.00  52.45   6.33   \n",
       "\n",
       "   ...       40     41     42     43     44     45    46     47     48     49  \n",
       "0  ...    63.50  68.10  50.84  64.10  39.41  45.10  46.0  53.74  69.10  43.88  \n",
       "2  ...    30.00  65.10  57.20  30.00  50.76  47.08  45.1  14.84  60.10  60.50  \n",
       "4  ...    68.52   4.00  34.77  50.85  54.04  55.33  57.9  61.00  41.38  44.00  \n",
       "6  ...    50.30  64.03  48.96  58.30  50.10  63.52  67.1  47.38  62.93  71.00  \n",
       "7  ...    32.01  28.50  42.94  50.20  43.98  55.07  63.0  38.50  65.01  67.52  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(df2)) < 0.8\n",
    "train = df2[msk]\n",
    "test = df2[~msk]\n",
    "train\n",
    "# then normalize\n",
    "# then write to train.csv, test.csv df2.to_csv(...)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.00000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.00000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49.451000</td>\n",
       "      <td>49.661800</td>\n",
       "      <td>50.469200</td>\n",
       "      <td>49.572733</td>\n",
       "      <td>48.333333</td>\n",
       "      <td>49.72480</td>\n",
       "      <td>50.167533</td>\n",
       "      <td>49.962933</td>\n",
       "      <td>50.616200</td>\n",
       "      <td>50.582533</td>\n",
       "      <td>...</td>\n",
       "      <td>49.260800</td>\n",
       "      <td>50.301000</td>\n",
       "      <td>48.182733</td>\n",
       "      <td>50.816600</td>\n",
       "      <td>50.154667</td>\n",
       "      <td>48.505667</td>\n",
       "      <td>49.065933</td>\n",
       "      <td>49.84940</td>\n",
       "      <td>49.457000</td>\n",
       "      <td>49.530733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.584529</td>\n",
       "      <td>12.427447</td>\n",
       "      <td>12.939042</td>\n",
       "      <td>12.791665</td>\n",
       "      <td>11.614033</td>\n",
       "      <td>12.46928</td>\n",
       "      <td>11.895658</td>\n",
       "      <td>12.916401</td>\n",
       "      <td>13.154818</td>\n",
       "      <td>13.038634</td>\n",
       "      <td>...</td>\n",
       "      <td>12.507019</td>\n",
       "      <td>11.998725</td>\n",
       "      <td>12.191564</td>\n",
       "      <td>12.511147</td>\n",
       "      <td>12.758544</td>\n",
       "      <td>13.445205</td>\n",
       "      <td>12.239636</td>\n",
       "      <td>12.15816</td>\n",
       "      <td>13.310002</td>\n",
       "      <td>14.457003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.800000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.99000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>14.35000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41.422500</td>\n",
       "      <td>43.770000</td>\n",
       "      <td>41.725000</td>\n",
       "      <td>41.655000</td>\n",
       "      <td>40.880000</td>\n",
       "      <td>42.59000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>43.815000</td>\n",
       "      <td>43.665000</td>\n",
       "      <td>43.172500</td>\n",
       "      <td>...</td>\n",
       "      <td>42.497500</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>42.025000</td>\n",
       "      <td>43.470000</td>\n",
       "      <td>42.710000</td>\n",
       "      <td>43.280000</td>\n",
       "      <td>42.812500</td>\n",
       "      <td>42.11750</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>43.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.735000</td>\n",
       "      <td>50.010000</td>\n",
       "      <td>50.240000</td>\n",
       "      <td>51.175000</td>\n",
       "      <td>47.650000</td>\n",
       "      <td>51.34500</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51.665000</td>\n",
       "      <td>52.450000</td>\n",
       "      <td>53.035000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.995000</td>\n",
       "      <td>49.700000</td>\n",
       "      <td>48.910000</td>\n",
       "      <td>51.440000</td>\n",
       "      <td>51.270000</td>\n",
       "      <td>48.810000</td>\n",
       "      <td>50.310000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>49.930000</td>\n",
       "      <td>50.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.890000</td>\n",
       "      <td>58.877500</td>\n",
       "      <td>61.442500</td>\n",
       "      <td>59.947500</td>\n",
       "      <td>57.197500</td>\n",
       "      <td>58.97500</td>\n",
       "      <td>59.425000</td>\n",
       "      <td>59.595000</td>\n",
       "      <td>60.100000</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.892500</td>\n",
       "      <td>60.527500</td>\n",
       "      <td>56.642500</td>\n",
       "      <td>60.675000</td>\n",
       "      <td>60.075000</td>\n",
       "      <td>58.980000</td>\n",
       "      <td>57.857500</td>\n",
       "      <td>59.00500</td>\n",
       "      <td>59.847500</td>\n",
       "      <td>59.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>78.900000</td>\n",
       "      <td>71.750000</td>\n",
       "      <td>70.300000</td>\n",
       "      <td>70.100000</td>\n",
       "      <td>72.520000</td>\n",
       "      <td>71.15000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>77.510000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>73.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>70.570000</td>\n",
       "      <td>78.900000</td>\n",
       "      <td>69.980000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>70.400000</td>\n",
       "      <td>85.050000</td>\n",
       "      <td>72.210000</td>\n",
       "      <td>77.15000</td>\n",
       "      <td>71.670000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4          5   \\\n",
       "count  150.000000  150.000000  150.000000  150.000000  150.000000  150.00000   \n",
       "mean    49.451000   49.661800   50.469200   49.572733   48.333333   49.72480   \n",
       "std     13.584529   12.427447   12.939042   12.791665   11.614033   12.46928   \n",
       "min      4.800000   12.400000    5.000000    5.000000    5.000000    4.99000   \n",
       "25%     41.422500   43.770000   41.725000   41.655000   40.880000   42.59000   \n",
       "50%     50.735000   50.010000   50.240000   51.175000   47.650000   51.34500   \n",
       "75%     59.890000   58.877500   61.442500   59.947500   57.197500   58.97500   \n",
       "max     78.900000   71.750000   70.300000   70.100000   72.520000   71.15000   \n",
       "\n",
       "               6           7           8           9      ...              40  \\\n",
       "count  150.000000  150.000000  150.000000  150.000000     ...      150.000000   \n",
       "mean    50.167533   49.962933   50.616200   50.582533     ...       49.260800   \n",
       "std     11.895658   12.916401   13.154818   13.038634     ...       12.507019   \n",
       "min     10.000000    4.990000    8.400000    5.000000     ...        4.000000   \n",
       "25%     43.100000   43.815000   43.665000   43.172500     ...       42.497500   \n",
       "50%     50.000000   51.665000   52.450000   53.035000     ...       50.995000   \n",
       "75%     59.425000   59.595000   60.100000   59.750000     ...       58.892500   \n",
       "max     73.000000   77.510000   71.000000   73.600000     ...       70.570000   \n",
       "\n",
       "               41          42          43          44          45          46  \\\n",
       "count  150.000000  150.000000  150.000000  150.000000  150.000000  150.000000   \n",
       "mean    50.301000   48.182733   50.816600   50.154667   48.505667   49.065933   \n",
       "std     11.998725   12.191564   12.511147   12.758544   13.445205   12.239636   \n",
       "min      4.990000   12.400000   13.000000    7.000000    5.000000   12.400000   \n",
       "25%     43.000000   42.025000   43.470000   42.710000   43.280000   42.812500   \n",
       "50%     49.700000   48.910000   51.440000   51.270000   48.810000   50.310000   \n",
       "75%     60.527500   56.642500   60.675000   60.075000   58.980000   57.857500   \n",
       "max     78.900000   69.980000   73.000000   70.400000   85.050000   72.210000   \n",
       "\n",
       "              47          48          49  \n",
       "count  150.00000  150.000000  150.000000  \n",
       "mean    49.84940   49.457000   49.530733  \n",
       "std     12.15816   13.310002   14.457003  \n",
       "min     14.35000    5.000000    4.800000  \n",
       "25%     42.11750   42.000000   43.430000  \n",
       "50%     50.00000   49.930000   50.490000  \n",
       "75%     59.00500   59.847500   59.697500  \n",
       "max     77.15000   71.670000   75.000000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/CNN_W2P_model/valid-1.csv', header = None) #, names = CSV_COLUMNS)\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/content/datalab/energy_forcasing/Sarah/trained/CNN_W2P_model’: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "mkdir /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "touch /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TIMESERIES_COL = 'price'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "SEQ_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
    "    SEQ_LEN = hparams['sequence_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
    "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
    "\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "    X = tf.reshape(features[TIMESERIES_COL],\n",
    "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2]\n",
    "    c2flat = tf.reshape(p2, [-1, outlen])\n",
    "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rnn_model(features, mode, params):\n",
    "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
    "\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 2-layer RNN\n",
    "def rnn2_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'state' is now a tuple containing the final state of each cell layer\n",
    "    # we use state[1] below to extract the final state of the final layer\n",
    "    \n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# create N-1 predictions\n",
    "def rnnN_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'outputs' contains the state of the final layer for every time step\n",
    "    # not just the last time step (?,N_INPUTS, final cell size)\n",
    "    \n",
    "    # 3. pass state for each time step through a DNN, to get a prediction\n",
    "    # for each time step \n",
    "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
    "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "    return predictions # return prediction for each time step\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    if predictions.shape[1] == 1:\n",
    "        loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        return loss, rmse\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        return loss, rmse\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def sequence_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'cnn': cnn_model,\n",
    "        'rnn': rnn_model,\n",
    "        'rnn2': rnn2_model,\n",
    "        'rnnN': rnnN_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    predictions = model_function(features, mode, params)\n",
    "\n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, rmse = compute_errors(features, labels, predictions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "        }\n",
    "\n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           save_checkpoints_secs=\n",
    "                                           hparams['min_eval_frequency']),\n",
    "                                       model_dir=output_dir)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      throttle_secs=hparams['min_eval_frequency'])\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--sequence_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "BIG_q.ipynb\t\tCNN_W2P_model  Energy_ts       train-1.csv  valid-1.csv\n",
      "Build_model_task.ipynb\tdata\t       sarah_data.csv  trained\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls\n",
    "#!{PWD}/CNN_W2P_model/trainer\n",
    "\n",
    "## /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the model locally on the vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--train_data_path=/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/train-1.csv', u'--eval_data_path=/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/valid-1.csv', u'--output_dir=/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained', u'--model=linear', u'--train_steps=1000', u'--sequence_length=50'], u'job_name': u'trainer.task'}, u'task': {}}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 60, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f141076ad50>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 60 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-09-18 19:09:43.909648: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 25079.137, step = 1\n",
      "INFO:tensorflow:global_step/sec: 57.1095\n",
      "INFO:tensorflow:loss = 289.77744, step = 101 (1.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.7371\n",
      "INFO:tensorflow:loss = 224.30597, step = 201 (1.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.6205\n",
      "INFO:tensorflow:loss = 175.8102, step = 301 (1.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2213\n",
      "INFO:tensorflow:loss = 143.65692, step = 401 (1.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.9526\n",
      "INFO:tensorflow:loss = 122.36126, step = 501 (1.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4378\n",
      "INFO:tensorflow:loss = 107.79161, step = 601 (1.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.9976\n",
      "INFO:tensorflow:loss = 97.50423, step = 701 (1.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2417\n",
      "INFO:tensorflow:loss = 90.07226, step = 801 (1.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 58.8636\n",
      "INFO:tensorflow:loss = 84.62491, step = 901 (1.699 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 80.63686.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-18-19:10:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-18-19:10:01\n",
      "INFO:tensorflow:Saving dict for global step 1000: RMSE = 18.543945, RMSE_same_as_last = 17.896685, global_step = 1000, loss = 343.87793\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/export/exporter/temp-1537297801/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%bash\n",
    "\n",
    "DATADIR=$(pwd)/data/CNN_W2P_model \n",
    "OUTDIR=$(pwd)/CNN_W2P_model/trained\n",
    "SEQ_LEN=50\n",
    "rm -rf $OUTDIR\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/train-1.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/valid-1.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=linear --train_steps=1000  --sequence_length=$SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/data/CNN_W2P_model ', ignore_errors=True)\n",
    "os.makedirs('/data/CNN_W2P_model/')\n",
    "np.random.seed(1) # makes data generation reproducible\n",
    "for i in xrange(0,10):\n",
    "  to_csv('data/CNN_W2P_model/train-{}.csv'.format(i), N = 1000, J = 1000)  # 1000 sequences\n",
    "  to_csv('data/CNN_W2P_model/valid-{}.csv'.format(i), N = 250, J = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://sarah-bucket/CNN_W2P_model/train-0.csv#1537231434174486...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-1.csv#1537231434240523...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-2.csv#1537231434253975...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-3.csv#1537231434272822...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-4.csv#1537231434297210...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-5.csv#1537231434202281...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-6.csv#1537231434336135...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-7.csv#1537231434187721...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-8.csv#1537231434315171...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/train-9.csv#1537231434333984...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-0.csv#1537231434311336...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-1.csv#1537231434300374...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-2.csv#1537231434531685...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-3.csv#1537231434515526...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-4.csv#1537231434565026...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/#1537231585994069...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-5.csv#1537231434453659...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-6.csv#1537231434511674...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-7.csv#1537231434467913...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-8.csv#1537231434460390...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/checkpoint#1537231588325720...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/valid-9.csv#1537231434459900...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/eval/#1537231542819681...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/eval/events.out.tfevents.1537231542.cmle-training-8229029589281429193#1537231591911197...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231592/variables/variables.data-00000-of-00001#1537231601429312...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/packages/f20940e826f8c0b6367b567955c6a099f95eae8829a37254007536ee6a503964/trainer-0.0.0.tar.gz#1537231445099641...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/#1537231545330977...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231544/variables/variables.data-00000-of-00001#1537231553333233...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/#1537231545671367...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231544/#1537231552076830...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231544/variables/variables.index#1537231553760294...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231592/#1537231599691972...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231544/saved_model.pb#1537231552408423...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231592/saved_model.pb#1537231600287262...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231592/variables/#1537231600884703...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231544/variables/#1537231552840977...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/export/exporter/1537231592/variables/variables.index#1537231601776564...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/events.out.tfevents.1537231531.cmle-training-8229029589281429193#1537231589886792...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/graph.pbtxt#1537231533589258...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/model.ckpt-1.data-00000-of-00001#1537231537248843...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/model.ckpt-1.index#1537231537790915...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/model.ckpt-1.meta#1537231540042031...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/model.ckpt-1000.data-00000-of-00001#1537231586519280...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/model.ckpt-1000.index#1537231587121237...\n",
      "Removing gs://sarah-bucket/CNN_W2P_model/linear/model.ckpt-1000.meta#1537231589343754...\n",
      "/ [1/45 objects]   2% Done                                                      \r",
      "/ [2/45 objects]   4% Done                                                      \r",
      "/ [3/45 objects]   6% Done                                                      \r",
      "/ [4/45 objects]   8% Done                                                      \r",
      "/ [5/45 objects]  11% Done                                                      \r",
      "/ [6/45 objects]  13% Done                                                      \r",
      "/ [7/45 objects]  15% Done                                                      \r",
      "/ [8/45 objects]  17% Done                                                      \r",
      "/ [9/45 objects]  20% Done                                                      \r",
      "/ [10/45 objects]  22% Done                                                     \r",
      "/ [11/45 objects]  24% Done                                                     \r",
      "/ [12/45 objects]  26% Done                                                     \r",
      "/ [13/45 objects]  28% Done                                                     \r",
      "/ [14/45 objects]  31% Done                                                     \r",
      "/ [15/45 objects]  33% Done                                                     \r",
      "/ [16/45 objects]  35% Done                                                     \r",
      "/ [17/45 objects]  37% Done                                                     \r",
      "/ [18/45 objects]  40% Done                                                     \r",
      "/ [19/45 objects]  42% Done                                                     \r",
      "/ [20/45 objects]  44% Done                                                     \r",
      "/ [21/45 objects]  46% Done                                                     \r",
      "/ [22/45 objects]  48% Done                                                     \r",
      "/ [23/45 objects]  51% Done                                                     \r",
      "/ [24/45 objects]  53% Done                                                     \r",
      "/ [25/45 objects]  55% Done                                                     \r",
      "/ [26/45 objects]  57% Done                                                     \r",
      "/ [27/45 objects]  60% Done                                                     \r",
      "/ [28/45 objects]  62% Done                                                     \r",
      "/ [29/45 objects]  64% Done                                                     \r",
      "/ [30/45 objects]  66% Done                                                     \r",
      "/ [31/45 objects]  68% Done                                                     \r",
      "/ [32/45 objects]  71% Done                                                     \r",
      "/ [33/45 objects]  73% Done                                                     \r",
      "/ [34/45 objects]  75% Done                                                     \r",
      "/ [35/45 objects]  77% Done                                                     \r",
      "/ [36/45 objects]  80% Done                                                     \r",
      "/ [37/45 objects]  82% Done                                                     \r",
      "/ [38/45 objects]  84% Done                                                     \r",
      "/ [39/45 objects]  86% Done                                                     \r",
      "/ [40/45 objects]  88% Done                                                     \r",
      "/ [41/45 objects]  91% Done                                                     \r",
      "/ [42/45 objects]  93% Done                                                     \r",
      "/ [43/45 objects]  95% Done                                                     \r",
      "/ [44/45 objects]  97% Done                                                     \r",
      "/ [45/45 objects] 100% Done                                                     \r\n",
      "Operation completed over 45 objects.                                             \n",
      "Copying file://data/CNN_W2P_model/train-0.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-1.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-2.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-4.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-3.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-7.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-5.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-6.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/train-8.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/valid-0.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/valid-2.csv [Content-Type=text/csv]...\n",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "Copying file://data/CNN_W2P_model/train-9.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/valid-3.csv [Content-Type=text/csv]...\n",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "Copying file://data/CNN_W2P_model/valid-1.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/valid-4.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/valid-5.csv [Content-Type=text/csv]...\n",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "Copying file://data/CNN_W2P_model/valid-6.csv [Content-Type=text/csv]...\n",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "Copying file://data/CNN_W2P_model/valid-8.csv [Content-Type=text/csv]...\n",
      "Copying file://data/CNN_W2P_model/valid-7.csv [Content-Type=text/csv]...\n",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "Copying file://data/CNN_W2P_model/valid-9.csv [Content-Type=text/csv]...\n",
      "/ [0/20 files][    0.0 B/ 66.6 MiB]   0% Done                                   \r",
      "/ [1/20 files][ 30.6 MiB/ 66.6 MiB]  45% Done                                   \r",
      "/ [2/20 files][ 30.6 MiB/ 66.6 MiB]  45% Done                                   \r",
      "/ [3/20 files][ 30.6 MiB/ 66.6 MiB]  45% Done                                   \r",
      "/ [4/20 files][ 30.6 MiB/ 66.6 MiB]  45% Done                                   \r",
      "-\r",
      "- [5/20 files][ 31.6 MiB/ 66.6 MiB]  47% Done                                   \r",
      "- [6/20 files][ 32.1 MiB/ 66.6 MiB]  48% Done                                   \r",
      "- [7/20 files][ 32.4 MiB/ 66.6 MiB]  48% Done                                   \r",
      "- [8/20 files][ 32.4 MiB/ 66.6 MiB]  48% Done                                   \r",
      "- [9/20 files][ 32.4 MiB/ 66.6 MiB]  48% Done                                   \r",
      "- [10/20 files][ 32.9 MiB/ 66.6 MiB]  49% Done                                  \r",
      "- [11/20 files][ 33.1 MiB/ 66.6 MiB]  49% Done                                  \r",
      "- [12/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "- [13/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "- [14/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "- [15/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "\\\r",
      "\\ [16/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "\\ [17/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "\\ [18/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "\\ [19/20 files][ 66.6 MiB/ 66.6 MiB]  99% Done                                  \r",
      "\\ [20/20 files][ 66.6 MiB/ 66.6 MiB] 100% Done                                  \r\n",
      "Operation completed over 20 objects/66.6 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil -m rm -rf gs://${BUCKET}/CNN_W2P_model/*\n",
    "gsutil -m cp data/CNN_W2P_model/*.csv gs://${BUCKET}/CNN_W2P_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push out to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_W2P_model_linear180918_191031\n",
      "jobId: CNN_W2P_model_linear180918_191031\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [CNN_W2P_model_linear180918_191031] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe CNN_W2P_model_linear180918_191031\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs CNN_W2P_model_linear180918_191031\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for MODEL in linear dnn cnn rnn rnn2 rnnN; do\n",
    "  OUTDIR=gs://${BUCKET}/CNN_W2P_model/${MODEL}\n",
    "  JOBNAME=CNN_W2P_model_${MODEL}$(date -u +%y%m%d_%H%M%S)\n",
    "  SEQ_LEN=50\n",
    "  REGION=us-central1\n",
    "  gsutil -m rm -rf $OUTDIR\n",
    "  echo $JOBNAME\n",
    "  gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "     --region=$REGION \\\n",
    "     --module-name=trainer.task \\\n",
    "     --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "     --job-dir=$OUTDIR \\\n",
    "     --scale-tier=BASIC \\\n",
    "     --runtime-version=$TFVERSION \\\n",
    "     -- \\\n",
    "     --train_data_path=\"gs://${BUCKET}/CNN_W2P_model/train-1.csv\" \\\n",
    "     --eval_data_path=\"gs://${BUCKET}/CNN_W2P_model/valid-1.csv\"  \\\n",
    "     --output_dir=$OUTDIR \\\n",
    "     --train_steps=1000 --sequence_length=$SEQ_LEN --model=$MODEL\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/CNN_W2P_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tensorboard --logdir=gs://{}/CNN_W2P_model --port=8083'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
